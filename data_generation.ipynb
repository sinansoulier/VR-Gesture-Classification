{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation - Variational Auto Encoder (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vr_recognition.data import Data\n",
    "from vr_recognition.hardware import Hardware\n",
    "from vr_recognition.model_utils import ModelUtils\n",
    "from vr_recognition.vr_gesture_recognizer import VRGestureRecognizer\n",
    "from vr_recognition.hardware import Hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir: str = os.path.join(os.getcwd(), 'simulated_data_v2')\n",
    "\n",
    "tuple_data: tuple[list[pd.DataFrame], np.ndarray, np.ndarray] = Data.load_data(base_dir)\n",
    "\n",
    "# Unpack data\n",
    "pd_data: list[pd.DataFrame] = tuple_data[0]\n",
    "labels: np.ndarray = tuple_data[1]\n",
    "classes: np.ndarray = tuple_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data: np.ndarray = Data.convert_to_numpy(pd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Class defining the encoder architecture of the Variational Autoencoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim: int = 2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, hidden_dim, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(hidden_dim, hidden_dim * 2, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(hidden_dim * 2, hidden_dim * 2, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Activation functions\n",
    "        self.relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "        # Dropout layer(s)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model. This method is called when performing inference, training or evaluation.\n",
    "        Params:\n",
    "            x (torch.Tensor): Input data\n",
    "        Returns:\n",
    "            torch.Tensor: Output data.\n",
    "                          The shape of the output tensor depends on the number of classes that characterizes the model.\n",
    "        \"\"\"\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        encoded = self.relu(self.conv4(x))\n",
    "\n",
    "        encoded = self.dropout(encoded)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trim(nn.Module):\n",
    "    \"\"\"\n",
    "    Class that defines a trimming layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, size1: int, size2: int):\n",
    "        super(Trim, self).__init__()\n",
    "        self.size1 = size1\n",
    "        self.size2 = size2\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the layer. This method is called when performing inference, training or evaluation.\n",
    "        Params:\n",
    "            x (torch.Tensor): Input data\n",
    "        Returns:\n",
    "            torch.Tensor: Trimmed data.\n",
    "        \"\"\"\n",
    "        return x[:, :, :self.size1, :self.size2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Class defining the decoder architecture of the VAE.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim: int = 2, img_shape: tuple[int, int] = (72, 114)):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(hidden_dim * 2, hidden_dim * 2, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(hidden_dim * 2, hidden_dim, kernel_size=3, stride=2, padding=1, output_padding=0)\n",
    "        self.deconv3 = nn.ConvTranspose2d(hidden_dim, hidden_dim, kernel_size=3, stride=2, padding=1, output_padding=0)\n",
    "        self.deconv4 = nn.ConvTranspose2d(hidden_dim, 1, kernel_size=3, stride=2, padding=1, output_padding=0)\n",
    "\n",
    "        # Activation functions\n",
    "        self.relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "        # Trimming\n",
    "        self.trim = Trim(img_shape[0], img_shape[1])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "        Returns:\n",
    "            torch.Tensor: Decoded tensor representing the reconstructed data.\n",
    "        \"\"\"\n",
    "        x = self.relu(self.deconv1(x))\n",
    "        x = self.relu(self.deconv2(x))\n",
    "        x = self.relu(self.deconv3(x))\n",
    "        decoded = self.relu(self.deconv4(x))\n",
    "\n",
    "        decoded = self.trim(decoded)\n",
    "        \n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Conditional Variational Autoencoder (CCVAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Log-variance:**</u><br><br>\n",
    "Instead of learning the variance $\\sigma^2$, we here learn the log-variance $\\log(\\sigma^2)$ because it can take any real number, while the variance can only take positive values. We can then exponentiate the log-variance to get the variance. Here is how we can do this:<br>\n",
    "\n",
    "$$\\log(\\sigma^2) = 2 * \\log(\\sigma)$$\n",
    "$$\\log(\\sigma^2) / 2 = \\log(\\sigma)$$\n",
    "$$e^{\\log(\\sigma^2) / 2} = \\sigma$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = Hardware.device()\n",
    "\n",
    "class CCVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Class-Conditional Variational AutoEncoder class.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim: int = 128, latent_dim: int = 2, input_shape: tuple = (72, 114), nb_classes: int = 4):\n",
    "        super(CCVAE, self).__init__()\n",
    "        self.encoder = Encoder(hidden_dim=hidden_dim)\n",
    "        self.decoder = Decoder(hidden_dim=hidden_dim, img_shape=input_shape)\n",
    "\n",
    "        self.nb_features = hidden_dim * 2\n",
    "        self.intermediate_shape = (math.ceil(input_shape[0] / (2 ** 4)), math.ceil(input_shape[1] / (2 ** 4)))\n",
    "\n",
    "        intermediary_dim = self.nb_features * self.intermediate_shape[0] * self.intermediate_shape[1]\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(intermediary_dim + nb_classes, hidden_dim)\n",
    "        self.mu_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.logvar_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc2 = nn.Linear(latent_dim + nb_classes, intermediary_dim)\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.nb_classes = nb_classes\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        sigma = torch.exp(logvar * 0.5).to(DEVICE)\n",
    "        epsilon = torch.randn_like(sigma).to(DEVICE)\n",
    "\n",
    "        return mu + epsilon * sigma\n",
    "    \n",
    "    def encode(self, x, y):\n",
    "        encoded = self.encoder(x)\n",
    "        flattened_encoded = encoded.view(x.size(0), -1)\n",
    "        \n",
    "        flattened_encoded = torch.cat((flattened_encoded, y), dim=1)\n",
    "        hidden = self.fc1(flattened_encoded)\n",
    "\n",
    "        return hidden\n",
    "    \n",
    "    def decode(self, z, y):\n",
    "        z = torch.cat((z, y), dim=1)\n",
    "        z = self.fc2(z)\n",
    "\n",
    "        reshaped_z = z.view(z.size(0),\n",
    "                            self.nb_features,\n",
    "                            self.intermediate_shape[0],\n",
    "                            self.intermediate_shape[1])\n",
    "\n",
    "        return self.decoder(reshaped_z)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        y_encoded = F.one_hot(y, num_classes=self.nb_classes).float().to(DEVICE)\n",
    "        encoded = self.encode(x, y_encoded)\n",
    "\n",
    "        mu, logvar = self.mu_layer(encoded), self.logvar_layer(encoded)\n",
    "\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "\n",
    "        decoded = self.decode(z, y_encoded)\n",
    "\n",
    "        return decoded, mu, logvar\n",
    "\n",
    "    def loss_function(self, x, x_hat, mu, logvar):\n",
    "        reconstruction_loss = nn.MSELoss()(x_hat, x)\n",
    "        kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        return reconstruction_loss + kld_loss\n",
    "    \n",
    "    def sample(self, n):\n",
    "        with torch.no_grad():\n",
    "            y = torch.randint(0, self.nb_classes, (n, 1)).to(DEVICE)\n",
    "            y_encoded = F.one_hot(y, num_classes=self.nb_classes).float().squeeze(1)\n",
    "            z = torch.randn(n, self.latent_dim).to(DEVICE)\n",
    "            decoded = self.decode(z, y_encoded)\n",
    "        return decoded.cpu().detach().numpy(), y.squeeze(-1).cpu().detach().numpy()\n",
    "    \n",
    "    def fit(self, X_train, X_val, y_train, y_val ,epochs: int = 10, learning_rate: float = 1e-3, batch_size: int = 32):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        n_train_batches = X_train.shape[0] // batch_size\n",
    "        n_val_batches = X_val.shape[0] // batch_size\n",
    "\n",
    "        history = pd.DataFrame(columns=['loss', 'val_loss'])\n",
    "\n",
    "        print('Training on {} batches, validating on {} batches'.format(n_train_batches, n_val_batches))\n",
    "\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            mean_loss = 0\n",
    "            self.train()\n",
    "            for x, y in Data.labeled_data_generator(X_train, y_train, batch_size=batch_size):\n",
    "                x = x.to(DEVICE).unsqueeze(1)\n",
    "                y = y.to(DEVICE)\n",
    "                x_hat, mu, logvar = self(x, y)\n",
    "                loss = self.loss_function(x, x_hat, mu, logvar)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                mean_loss += loss.item()\n",
    "            \n",
    "            mean_loss /= n_train_batches\n",
    "\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                mean_val_loss = 0\n",
    "                for x, y in Data.labeled_data_generator(X_val, y_val, batch_size=batch_size):\n",
    "                    x = x.to(DEVICE).unsqueeze(1)\n",
    "                    y = y.to(DEVICE)\n",
    "                    x_hat, mu, logvar = self(x, y)\n",
    "                    val_loss = self.loss_function(x, x_hat, mu, logvar)\n",
    "\n",
    "                    mean_val_loss += val_loss.item()\n",
    "\n",
    "                mean_val_loss /= n_val_batches\n",
    "                print('epoch [{}/{}] loss: {:.2f} | validation Loss: {:.2f}                         '.format(epoch+1, epochs, mean_loss, mean_val_loss), end='\\r')\n",
    "\n",
    "            history.loc[epoch] = [mean_loss, mean_val_loss]\n",
    "        \n",
    "        self.history = history\n",
    "\n",
    "    def evaluate(self, X_test, y_test, batch_size: int = 32):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            test_mean_loss = 0\n",
    "            n_test_batches = X_test.shape[0] // batch_size\n",
    "            for x, y in Data.labeled_data_generator(X_test, y_test, batch_size=batch_size):\n",
    "                x = x.to(DEVICE).unsqueeze(1)\n",
    "                y = y.to(DEVICE)\n",
    "                x_hat, z_mu, z_logvar = self(x, y)\n",
    "                test_mean_loss = self.loss_function(x_hat, x, z_mu, z_logvar)\n",
    "                test_mean_loss += test_mean_loss.item()\n",
    "        \n",
    "        test_mean_loss = test_mean_loss / n_test_batches\n",
    "        print(f\"Test loss: {test_mean_loss:.4f}\")\n",
    "    \n",
    "    def plot_history(self):\n",
    "        plt.plot(self.history['loss'], label='loss')\n",
    "        plt.plot(self.history['val_loss'], label='val_loss')\n",
    "        plt.legend(['Loss', 'Validation Loss'])\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed: int = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.mps.manual_seed(seed)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=seed)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=seed)\n",
    "\n",
    "ccvae = CCVAE(hidden_dim=32, latent_dim=32, input_shape=data.shape[1:]).to(DEVICE)\n",
    "ccvae.fit(X_train, X_val, y_train, y_val, epochs=950, learning_rate=1e-3, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### History - Training & Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccvae.plot_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - CCVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccvae.evaluate(X_test, y_test, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 5000\n",
    "generation_seed = 90\n",
    "\n",
    "torch.manual_seed(generation_seed)\n",
    "X_gen, y_gen = ccvae.sample(n=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on previously trained classification model\n",
    "# classified_output = ModelUtils.run_pytorch_inference(path=\"models/random_forest.onnx\", input=X_gen)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=45)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=512,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_classifier.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "print(\"Test accuracy: {:.2f}%\".format(rf_classifier.score(X_test.reshape(X_test.shape[0], -1), y_test) * 100))\n",
    "\n",
    "classified_output = rf_classifier.predict(X_gen.reshape(X_gen.shape[0], -1))\n",
    "\n",
    "accuracy = accuracy_score(y_gen, classified_output)\n",
    "print(f\"Generation accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Keep only the correctly classified samples\n",
    "X_gen_c = X_gen[classified_output == y_gen]\n",
    "y_gen_c = y_gen[classified_output == y_gen]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving newly generated samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(\"fake_data\", \"X_gen.npy\"), X_gen_c)\n",
    "np.save(os.path.join(\"fake_data\", \"y_gen.npy\"), y_gen_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = Hardware.device()\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_LABELS = classes.shape[0]\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "gesture_recognizer = VRGestureRecognizer(hidden_size=HIDDEN_SIZE, num_classes=NUM_LABELS).to(DEVICE)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_gen_c.squeeze(1), y_gen_c, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "gesture_recognizer.compile(loss_fn=loss_fn)\n",
    "training_history = gesture_recognizer.fit(X_train, y_train, X_val, y_val, epochs=EPOCHS, batch_size=BATCH_SIZE, learning_rate=LEARNING_RATE)\n",
    "\n",
    "gesture_recognizer.plot_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on real data\n",
    "test_loss, test_accuracy = gesture_recognizer.to(Hardware.device()).evaluate(data, labels)\n",
    "print(f\"Loss: {test_loss.item():.2f} | Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('models'):\n",
    "    os.mkdir('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_recognizer.export_to_onnx(path='models/vr_artifical_gesture_recognizer.onnx', data_shape=data.shape[1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SCIA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
