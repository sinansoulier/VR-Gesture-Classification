{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VR - Motion recognition with simple gestures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "* Numpy\n",
    "* Pandas\n",
    "* Matplotlib\n",
    "* PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Generator, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base data directory\n",
    "base_dir: str = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "# Load data\n",
    "tuple_data: tuple[np.ndarray, np.ndarray, np.ndarray] = Data.load_data(base_dir)\n",
    "\n",
    "# Unpack data\n",
    "data: np.ndarray = tuple_data[0]\n",
    "labels: np.ndarray = tuple_data[1]\n",
    "classes: np.ndarray = tuple_data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(X: np.ndarray, y: np.ndarray, batch_size: int=32) -> Generator[Tuple[torch.Tensor, torch.Tensor], None, None]:\n",
    "    \"\"\"\n",
    "    Data generator for PyTorch.\n",
    "    Params:\n",
    "        X (np.ndarray): Data\n",
    "        y (np.ndarray): Labels\n",
    "        batch_size (int): Size of the batch, default to 32\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]: Tuple containing the data and the labels\n",
    "    \"\"\"\n",
    "    # Number of samples\n",
    "    n_samples: int = X.shape[0]\n",
    "    \n",
    "    # Shuffle indices\n",
    "    indices: np.ndarray = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Shuffle data\n",
    "    X: np.ndarray = X[indices]\n",
    "    y: np.ndarray = y[indices]\n",
    "    \n",
    "    # Iterate over the dataset\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        # Get batch data\n",
    "        X_batch = X[i:i+batch_size]\n",
    "        y_batch = y[i:i+batch_size]\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        X_batch = torch.from_numpy(X_batch)\n",
    "        y_batch = torch.from_numpy(y_batch)\n",
    "        \n",
    "        # Yield the batch\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRGestureRecognizer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(VRGestureRecognizer, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_size, 64, kernel_size=3, stride=1)\n",
    "        # self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        # self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(64*4*4, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 32)\n",
    "        self.fc2 = nn.Linear(32, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.maxpool(out)\n",
    "        # out = self.relu(self.conv2(out))\n",
    "        # out = self.maxpool(out)\n",
    "        # out = self.relu(self.conv3(out))\n",
    "        # out = self.maxpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.relu(self.fc1(out))\n",
    "        # out = self.fc2(out)\n",
    "        # out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def device() -> str:\n",
    "    \"\"\"\n",
    "    Returns the device to use for training.\n",
    "    Returns:\n",
    "        device: str - Device to use for training\n",
    "    \"\"\"\n",
    "    # Define CPU as default device\n",
    "    device = \"cpu\"\n",
    "\n",
    "    # Use Cuda acceleration if available (Nvidia GPU)\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "    # Use Metal acceleration if available (MacOS)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps:0\"\n",
    "    \n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 10\n",
    "EPOCHS = 10\n",
    "\n",
    "gesture_recognizer = VRGestureRecognizer(BATCH_SIZE, 128, classes.shape[0]).to(device())\n",
    "optimizer = torch.optim.Adam(gesture_recognizer.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.15, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, batch in enumerate(data_generator(X_train, y_train, batch_size=BATCH_SIZE)):\n",
    "        # Get batch data\n",
    "        X_batch, y_batch = batch\n",
    "        X_batch, y_batch = X_batch.to(device()), y_batch.to(device())\n",
    "        # Forward pass\n",
    "        outputs = gesture_recognizer(X_batch)\n",
    "        print(outputs.size())\n",
    "        loss = criterion(outputs, y_batch.long())\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{EPOCHS}], Step [{i+1}/{len(X_batch)//BATCH_SIZE}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:56].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SCIA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
