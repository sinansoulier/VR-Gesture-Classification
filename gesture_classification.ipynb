{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VR - Motion recognition with simple gestures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "* Numpy\n",
    "* Pandas\n",
    "* Matplotlib\n",
    "* PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Generator, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base data directory\n",
    "base_dir: str = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "# Load data\n",
    "tuple_data: tuple[np.ndarray, np.ndarray, np.ndarray] = Data.load_data(base_dir)\n",
    "\n",
    "# Unpack data\n",
    "data: np.ndarray = tuple_data[0]\n",
    "labels: np.ndarray = tuple_data[1]\n",
    "classes: np.ndarray = tuple_data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(X: np.ndarray, y: np.ndarray, batch_size: int=32) -> Generator:\n",
    "    \"\"\"\n",
    "    Data generator for PyTorch.\n",
    "    Params:\n",
    "        X (np.ndarray): Data\n",
    "        y (np.ndarray): Labels\n",
    "        batch_size (int): Size of the batch, default to 32\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]: Tuple containing the data and the labels\n",
    "    \"\"\"\n",
    "    # Number of samples\n",
    "    n_samples: int = X.shape[0]\n",
    "    \n",
    "    # Shuffle indices\n",
    "    indices: np.ndarray = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Shuffle data\n",
    "    X: np.ndarray = X[indices]\n",
    "    y: np.ndarray = y[indices]\n",
    "    \n",
    "    # Iterate over the dataset\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        # Get batch data\n",
    "        X_batch = X[i:i+batch_size]\n",
    "        y_batch = y[i:i+batch_size]\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        X_batch = torch.from_numpy(X_batch)\n",
    "        y_batch = torch.from_numpy(y_batch)\n",
    "        \n",
    "        # Yield the batch\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRGestureRecognizer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(VRGestureRecognizer, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(256*4*7, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.maxpool(self.relu(self.conv1(x)))\n",
    "        out = self.maxpool(self.relu(self.conv2(out)))\n",
    "        out = self.maxpool(self.relu(self.conv3(out)))\n",
    "        out = self.maxpool(self.relu(self.conv4(out)))\n",
    "        # print(out.size())\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.relu(self.fc1(out))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.softmax(self.fc3(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def device() -> str:\n",
    "    \"\"\"\n",
    "    Returns the device to use for training.\n",
    "    Returns:\n",
    "        device: str - Device to use for training\n",
    "    \"\"\"\n",
    "    # Define CPU as default device\n",
    "    device = \"cpu\"\n",
    "\n",
    "    # Use Cuda acceleration if available (Nvidia GPU)\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "    # Use Metal acceleration if available (MacOS)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps:0\"\n",
    "    \n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] | Loss: 1.54 | Accuracy: 0.30\n",
      "Epoch [2/50] | Loss: 1.51 | Accuracy: 0.30\n",
      "Epoch [3/50] | Loss: 1.43 | Accuracy: 0.32\n",
      "Epoch [4/50] | Loss: 1.31 | Accuracy: 0.44\n",
      "Epoch [5/50] | Loss: 1.11 | Accuracy: 0.44\n",
      "Epoch [6/50] | Loss: 1.25 | Accuracy: 0.54\n",
      "Epoch [7/50] | Loss: 1.05 | Accuracy: 0.56\n",
      "Epoch [8/50] | Loss: 1.14 | Accuracy: 0.56\n",
      "Epoch [9/50] | Loss: 1.13 | Accuracy: 0.54\n",
      "Epoch [10/50] | Loss: 1.33 | Accuracy: 0.56\n",
      "Epoch [11/50] | Loss: 1.22 | Accuracy: 0.56\n",
      "Epoch [12/50] | Loss: 1.32 | Accuracy: 0.56\n",
      "Epoch [13/50] | Loss: 1.21 | Accuracy: 0.56\n",
      "Epoch [14/50] | Loss: 1.12 | Accuracy: 0.56\n",
      "Epoch [15/50] | Loss: 1.23 | Accuracy: 0.56\n",
      "Epoch [16/50] | Loss: 1.13 | Accuracy: 0.56\n",
      "Epoch [17/50] | Loss: 1.22 | Accuracy: 0.56\n",
      "Epoch [18/50] | Loss: 1.04 | Accuracy: 0.56\n",
      "Epoch [19/50] | Loss: 1.03 | Accuracy: 0.56\n",
      "Epoch [20/50] | Loss: 1.22 | Accuracy: 0.56\n",
      "Epoch [21/50] | Loss: 1.18 | Accuracy: 0.56\n",
      "Epoch [22/50] | Loss: 1.34 | Accuracy: 0.68\n",
      "Epoch [23/50] | Loss: 1.07 | Accuracy: 0.76\n",
      "Epoch [24/50] | Loss: 0.85 | Accuracy: 0.78\n",
      "Epoch [25/50] | Loss: 1.07 | Accuracy: 0.80\n",
      "Epoch [26/50] | Loss: 0.86 | Accuracy: 0.80\n",
      "Epoch [27/50] | Loss: 0.77 | Accuracy: 0.80\n",
      "Epoch [28/50] | Loss: 1.12 | Accuracy: 0.80\n",
      "Epoch [29/50] | Loss: 0.94 | Accuracy: 0.80\n",
      "Epoch [30/50] | Loss: 0.93 | Accuracy: 0.80\n",
      "Epoch [31/50] | Loss: 1.13 | Accuracy: 0.80\n",
      "Epoch [32/50] | Loss: 0.93 | Accuracy: 0.80\n",
      "Epoch [33/50] | Loss: 0.93 | Accuracy: 0.80\n",
      "Epoch [34/50] | Loss: 0.93 | Accuracy: 0.80\n",
      "Epoch [35/50] | Loss: 0.95 | Accuracy: 0.80\n",
      "Epoch [36/50] | Loss: 1.13 | Accuracy: 0.80\n",
      "Epoch [37/50] | Loss: 1.13 | Accuracy: 0.80\n",
      "Epoch [38/50] | Loss: 0.94 | Accuracy: 0.80\n",
      "Epoch [39/50] | Loss: 0.74 | Accuracy: 0.80\n",
      "Epoch [40/50] | Loss: 0.84 | Accuracy: 0.80\n",
      "Epoch [41/50] | Loss: 0.93 | Accuracy: 0.80\n",
      "Epoch [42/50] | Loss: 0.93 | Accuracy: 0.80\n",
      "Epoch [43/50] | Loss: 0.93 | Accuracy: 0.80\n",
      "Epoch [44/50] | Loss: 0.83 | Accuracy: 0.80\n",
      "Epoch [45/50] | Loss: 1.29 | Accuracy: 0.80\n",
      "Epoch [46/50] | Loss: 0.83 | Accuracy: 0.80\n",
      "Epoch [47/50] | Loss: 0.89 | Accuracy: 0.88\n",
      "Epoch [48/50] | Loss: 0.75 | Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 10\n",
    "EPOCHS = 50\n",
    "\n",
    "gesture_recognizer = VRGestureRecognizer(hidden_size=128, num_classes=classes.shape[0]).to(device(), dtype=torch.float32)\n",
    "optimizer = torch.optim.Adam(gesture_recognizer.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.15, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(EPOCHS):\n",
    "    mean_loss = 0\n",
    "    mean_acc = 0\n",
    "\n",
    "    data_gen =  data_generator(X_train, y_train, batch_size=BATCH_SIZE)\n",
    "    n_batches = 0\n",
    "    for i, batch in enumerate(data_gen):\n",
    "        n_batches += 1\n",
    "        # Get batch data\n",
    "        X_batch, y_batch = batch\n",
    "        X_batch, y_batch = X_batch.to(device()).unsqueeze(1), y_batch.to(device())\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = gesture_recognizer(X_batch)\n",
    "        loss = criterion(outputs, y_batch.long())\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update epoch loss\n",
    "        mean_loss += loss.item()\n",
    "        # Update epoch accuracy\n",
    "        mean_acc += outputs.to('cpu').argmax(dim=1).eq(y_batch.to('cpu')).sum().item()\n",
    "\n",
    "    # Compute mean loss and accuracy for the current epoch\n",
    "    mean_loss /= n_batches\n",
    "    mean_acc = mean_acc / (n_batches * BATCH_SIZE)\n",
    "    # Print epoch results\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] | Loss: {loss.item():.2f} | Accuracy: {mean_acc:.2f}\")\n",
    "    if mean_acc > 0.9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.95 | Accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X = torch.from_numpy(X).to(device()).unsqueeze(1)\n",
    "        y = torch.from_numpy(y).to(device())\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y.long())\n",
    "        acc = outputs.to('cpu').argmax(dim=1).eq(y.to('cpu')).sum().item() / X.shape[0]\n",
    "        print(f\"Loss: {loss.item():.2f} | Accuracy: {acc:.2f}\")\n",
    "\n",
    "evaluate(gesture_recognizer, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SCIA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
